{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 env & utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(28)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "env = gym.make(\"CartPole-v1\")    # 0 up, 1 right, 2 down, 3 left\n",
    "env.reset()\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_avg(a,n=100,mode=\"valid\"):\t\n",
    "    return(np.convolve(a, np.ones((n,))/n, mode=mode))\t\n",
    "    \n",
    "def plot_reward(model_name, rew): \n",
    "    plt.figure(figsize=[15, 6])\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    X = np.linspace(1,len(rew),len(rew))\n",
    "    plt.plot(X, rew)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.title(model_name)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    rew = move_avg(rew)\n",
    "    X = np.linspace(1,len(rew),len(rew))\n",
    "    plt.plot(X, rew)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.title(model_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent):\n",
    "    # train step\n",
    "    rew_total = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        obs = env.reset()[0]\n",
    "        done = False\n",
    "        final_reward = 0\n",
    "        while not done:\n",
    "            \n",
    "            # env.render()\n",
    "            loss = 0\n",
    "            v, a, a_logprob = agent.net(Variable(torch.FloatTensor(obs)).to(device))\n",
    "            obs_next, reward, done, _, info = env.step(a)\n",
    "            x, x_dot, theta, theta_dot = obs_next\n",
    "\n",
    "            r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "            r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "            reward = r1 + r2\n",
    "                \n",
    "            agent.add(v, a_logprob, reward)\n",
    "            final_reward += reward\n",
    "\n",
    "            # update network\n",
    "            if done:\n",
    "                loss = agent.learn()\n",
    "                print('Ep: ', epoch,' | reward:%.3f'%final_reward, ' | loss:%.4f'%loss)\n",
    "\n",
    "                rew_total.append(final_reward)\n",
    "            \n",
    "            obs = obs_next\n",
    "    return rew_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Algorithm\n",
    "\n",
    "Based on PolicyGradient, actor-critic was born. \n",
    "\n",
    "This is essentially a method that combines Policy Gradient (Actor) and Function Approximation (Critic). The Actor selects behaviors based on probability, the Critic evaluates the score of the behavior based on the Actor's behavior, and the Actor modifies the probability of selecting the behavior based on the Critic's score.\n",
    "\n",
    "Advantages: It can be updated in a single step, and the training efficiency is faster than Policy Gradient.\n",
    "\n",
    "Disadvantages: It depends on the value judgment of the Critic, but it is difficult for the Critic to converge. Coupled with the update of the Actor, it is even more difficult to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AC import ActorCritic\n",
    "agent = ActorCritic(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rl/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rew_total \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plot_reward(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActor-Critic_reward\u001b[39m\u001b[38;5;124m'\u001b[39m, rew_total)\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m v, a, a_logprob \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mnet(Variable(torch\u001b[38;5;241m.\u001b[39mFloatTensor(obs))\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 13\u001b[0m obs_next, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m     14\u001b[0m x, x_dot, theta, theta_dot \u001b[38;5;241m=\u001b[39m obs_next\n\u001b[1;32m     16\u001b[0m r1 \u001b[38;5;241m=\u001b[39m (env\u001b[38;5;241m.\u001b[39mx_threshold \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mabs\u001b[39m(x)) \u001b[38;5;241m/\u001b[39m env\u001b[38;5;241m.\u001b[39mx_threshold \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "rew_total = train(agent)\n",
    "plot_reward('Actor-Critic_reward', rew_total)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4e1b660db4d23c812760aa03cf44c44d867f40c2f26888e32ea14348e8f404a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
