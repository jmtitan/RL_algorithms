{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG\n",
    "Deep Deterministic Policy Gradient\n",
    "同时吸收了actor-critic单步更新和DQN价值导向的精华，合成为一个新算法。\n",
    " - deep: value-based的思想，即一个经验池和两个结构相同的深度神经网络促进学习。\n",
    " - Deterministic Policy Gradient: 相对于Policy Gradient改变了动作输出的过程，从生成分布并采样改为在连续的动作仅输出一个动作值。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. env & utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(28)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " State Dimensions :-  24\n",
      " Action Dimensions :-  4\n",
      " Action Max :-  1.0\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "MAX_STEPS = 2000\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v3\")    # 0 up, 1 right, 2 down, 3 left\n",
    "S_DIM = env.observation_space.shape[0]\n",
    "A_DIM = env.action_space.shape[0]\n",
    "A_MAX = env.action_space.high[0]\n",
    " \n",
    "print(' State Dimensions : ', S_DIM)\n",
    "print(' Action Dimensions : ', A_DIM)\n",
    "print(' Action Max : ', A_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\tobs = env.reset()\n",
    "\tfor r in range(MAX_STEPS):\n",
    "\n",
    "\t\t# env.render()\n",
    "\t\tstate = np.float32(obs)\n",
    "\n",
    "\t\taction = DDPG.get_exploration_action(state)\n",
    "\t\t# if epoch%5 == 0:\n",
    "\t\t# \t# validate every 5th episode\n",
    "\t\t# \taction = trainer.get_exploitation_action(state)\n",
    "\t\t# else:\n",
    "\t\t# \t# get action based on observation, use exploration policy here\n",
    "\t\t# \taction = trainer.get_exploration_action(state)\n",
    "\n",
    "\t\tobs_next, reward, done, info = env.step(action)\n",
    "\n",
    "\t\tif done:\n",
    "\t\t\tnew_state = None\n",
    "\t\telse:\n",
    "\t\t\tnew_state = np.float32(obs_next)\n",
    "\t\t\t# push this exp in ram\n",
    "\t\t\tram.add(state, action, reward, new_state)\n",
    "\n",
    "\t\tobs = obs_next\n",
    "\n",
    "\t\t# perform optimization\n",
    "\t\tDDPG.learn()\n",
    "\t\tif done:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# check memory consumption and clear memory\n",
    "\tgc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_buffer:\n",
    "\n",
    "    def __init__(self, N):\n",
    "\n",
    "        self.capacity = N\n",
    "        self.counter = 0\n",
    "        self.buf = deque(maxlen=self.capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.counter\n",
    "\n",
    "    def add(self, s1, a, r, s2):\n",
    "        transition = (s1, a, r, s2)\n",
    "        self.counter += 1\n",
    "        if self.counter > self.capacity:\n",
    "            self.counter = self.capacity\n",
    "        self.buf.append(transition)\n",
    "\n",
    "    def sample(self, minibatch):\n",
    "\n",
    "        batch_num = min(minibatch, self.counter)\n",
    "        batch = random.sample(self.buf, batch_num)\n",
    "\n",
    "        b_s1 = Variable(torch.FloatTensor(t[0] for t in batch))\n",
    "        b_a = Variable(torch.FloatTensor(t[1] for t in batch))\n",
    "        b_r = Variable(torch.FloatTensor(t[2] for t in batch))\n",
    "        b_s2 = Variable(torch.FloatTensor(t[3] for t in batch))\n",
    "        return b_s1, b_a, b_r, b_s2 \n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\n",
    "\t\tself.state_dim = state_dim\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\tself.feature_state = nn.Sequential(\n",
    "            nn.Linear(state_dim,256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\t\tself.feature_action = nn.Sequential(\n",
    "            nn.Linear(action_dim, 128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\t\tself.value = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\n",
    "\t\tf_state = self.feature_state(state)\n",
    "\t\tf_action = self.feature_action(action)\n",
    "\t\tval = torch.cat((f_state, f_action),dim=1)\n",
    "\n",
    "\t\tval = self.value(val)\n",
    "\n",
    "\t\treturn val\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "\tdef __init__(self, state_dim, action_dim, action_lim):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.state_dim = state_dim\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\tself.action_lim = action_lim\n",
    "\n",
    "\t\tself.feature = nn.Sequential(\n",
    "\t\t\tnn.Linear(state_dim,256),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.Linear(256, 128),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.Linear(128, 64),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t)\n",
    "\t\tself.classify = nn.Sequential(\n",
    "\t\t\tnn.Linear(64, action_dim),\n",
    "\t\t\tnn.Tanh()\n",
    "\t\t)\n",
    "\tdef forward(self, state):\n",
    "\t\t\n",
    "\t\tx = self.feature(state)\n",
    "\t\ta = self.classify(x)\n",
    "\n",
    "\t\taction = a * self.action_lim\n",
    "\n",
    "\t\treturn action\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4e1b660db4d23c812760aa03cf44c44d867f40c2f26888e32ea14348e8f404a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
